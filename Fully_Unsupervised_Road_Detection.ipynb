{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahatami/Unsupervised-Drivable-Area-Detection/blob/master/Fully_Unsupervised_Road_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I5YkQU2MqrZK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models.segmentation as seg\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import sys\n",
        "from collections import namedtuple\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from numpy.linalg import norm\n",
        "import math\n",
        "import datetime\n",
        "from torch import nn\n",
        "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, deeplabv3_resnet50, lraspp_mobilenet_v3_large\n",
        "\n",
        "torch.set_printoptions(threshold=10000)\n",
        "np.set_printoptions(threshold = np.inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyk_K9NzU3O-"
      },
      "source": [
        "Dataset for Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH6L0lC0xKEO"
      },
      "outputs": [],
      "source": [
        "! wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=sarahatami&password=**************&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "! wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=12 #sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wokPphH_qNj"
      },
      "outputs": [],
      "source": [
        "!unzip /content/leftImg8bit_demoVideo.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7Nx29T_RCf8w"
      },
      "outputs": [],
      "source": [
        "!cp /content/leftImg8bit/demoVideo/stuttgart_00/stuttgart_00_000000_000599_leftImg8bit.png /content/leftImg8bit/demoVideo/stuttgart_00/stuttgart_00_000000_000600_leftImg8bit.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8srJnrmV5rU",
        "outputId": "6d624f8e-91b7-4e31-e4f1-5de028c23ec9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def count_images(directory, extension):\n",
        "    return len([entry for entry in os.scandir(directory) if entry.is_file() and entry.name.endswith(extension)])\n",
        "directory = '/content/leftImg8bit/demoVideo/stuttgart_00'\n",
        "extension = '.png'\n",
        "count_images(directory, extension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hb0NXMQVDFW"
      },
      "source": [
        "Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TZoUVr6Nq689"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize((205, 410)),\n",
        "                                transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8Wst5AKLq9YC"
      },
      "outputs": [],
      "source": [
        "### Defining the Dataset class ###\n",
        "class CityscapesTrain(Dataset):\n",
        "    def __init__(self, transform=None, img_dir=None):\n",
        "        self.transform = transform\n",
        "        self.images_dir = img_dir\n",
        "        self.images = sorted(os.listdir(self.images_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(pil_image)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qKo9lSSUq-H8"
      },
      "outputs": [],
      "source": [
        "#Define Dataloader\n",
        "train_addr_img='/content/leftImg8bit/demoVideo/stuttgart_00'\n",
        "trainset = CityscapesTrain(transform=transform, img_dir=train_addr_img)\n",
        "batch_size = 6\n",
        "dataloader_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6B0dBkLyKqq"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjQOp50R7E3"
      },
      "source": [
        "HORIZON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P-No9SBiR6gL"
      },
      "outputs": [],
      "source": [
        "teta = 0.457\n",
        "horizon = int( 1354.98 - (2305.87 * math.tan(teta)) )\n",
        "horizon=90"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA2DKJQqVPin"
      },
      "source": [
        "4 Point Coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZfuQsjvnrBjz"
      },
      "outputs": [],
      "source": [
        "H = [[137.122\t, -282.815  ,  0],\n",
        "     [64.142\t,   0      , 345.458],\n",
        "     [1       ,   0      ,  0]]\n",
        "# Points are on the ground --> z=0\n",
        "world_point1= [2.6 ,0.7 ,1]\n",
        "world_point2= [2.6 ,-2 ,1]\n",
        "world_point3= [4 ,0.2 ,1]\n",
        "world_point4= [4 ,-2 ,1]\n",
        "\n",
        "u_v_w1= np.dot(H , world_point1)\n",
        "u1,v1= u_v_w1[0]/u_v_w1[2],u_v_w1[1]/u_v_w1[2]\n",
        "u1,v1=int(u1), int(v1)\n",
        "\n",
        "u_v_w2= np.dot(H , world_point2)\n",
        "u2,v2= u_v_w2[0]/u_v_w2[2],u_v_w2[1]/u_v_w2[2]\n",
        "u2,v2=int(u2), int(v2)\n",
        "\n",
        "u_v_w3= np.dot(H , world_point3)\n",
        "u3,v3= u_v_w3[0]/u_v_w3[2],u_v_w3[1]/u_v_w3[2]\n",
        "u3,v3=int(u3), int(v3)\n",
        "\n",
        "u_v_w4= np.dot(H , world_point4)\n",
        "u4,v4= u_v_w4[0]/u_v_w4[2],u_v_w4[1]/u_v_w4[2]\n",
        "u4,v4=int(u4), int(v4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KVix-qPrErq"
      },
      "outputs": [],
      "source": [
        "folder_dir='/content/leftImg8bit/demoVideo/stuttgart_00'\n",
        "s = 3\n",
        "for image in sorted(os.listdir(folder_dir)):\n",
        "  input_path = os.path.join(folder_dir, image)\n",
        "  im = Image.open(input_path).convert(\"RGB\") #<class 'PIL.Image.Image'>\n",
        "  im = transforms.Resize((205, 410))(im)\n",
        "\n",
        "  canvas = ImageDraw.Draw(im)\n",
        "  canvas.ellipse((u1-s, v1-s, u1+s, v1+s), fill=(255, 0, 0))\n",
        "  canvas.ellipse((u2-s, v2-s, u2+s, v2+s), fill=(255, 255, 0))\n",
        "  canvas.ellipse((u3-s, v3-s, u3+s, v3+s), fill=(0, 255, 0))\n",
        "  canvas.ellipse((u4-s, v4-s, u4+s, v4+s), fill=(0, 0, 200))\n",
        "  del canvas\n",
        "\n",
        "  plt.imshow(im)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR42oTgcrT81"
      },
      "source": [
        "ROAD MASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KXpeRQyCrKj1"
      },
      "outputs": [],
      "source": [
        "def create_road_mask(img: torch.tensor) -> torch.tensor:\n",
        "  img = img.cpu().detach().numpy()\n",
        "  h = img.shape[0]\n",
        "  w = img.shape[1]\n",
        "  label_map = np.zeros((h,w), dtype=np.uint8)\n",
        "\n",
        "  points = np.array([[u1, v1], [u3, v3], [u4, v4], [u2, v2]])\n",
        "  label_map = cv2.fillPoly(label_map, pts=[points], color=(1, 0, 0))\n",
        "\n",
        "  label_map = torch.from_numpy(label_map)\n",
        "  return label_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp08IWtjrZOn"
      },
      "source": [
        "Weight Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4bN1I6Vsrelx"
      },
      "outputs": [],
      "source": [
        "#CREATE WEIGHT MASK\n",
        "def create_weight_mask(img: torch.tensor) -> torch.tensor:\n",
        "  img = img.cpu().detach().numpy()\n",
        "  h = img.shape[0]\n",
        "  w = img.shape[1]\n",
        "  weight_map = np.zeros((h,w), dtype=np.uint8)\n",
        "\n",
        "  horizon_points = np.array([[0,0], [0, horizon], [w, horizon], [w, 0]])\n",
        "  weight_map = cv2.fillPoly(weight_map, pts=[horizon_points], color=(1, 0, 0))\n",
        "\n",
        "  road_points = np.array([[u1, v1], [u3, v3], [u4, v4], [u2, v2]])\n",
        "  weight_map = cv2.fillPoly(weight_map, pts=[road_points], color=(1, 0, 0))\n",
        "\n",
        "  weight_map = torch.from_numpy(weight_map)\n",
        "  return weight_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqcZv6OarQue"
      },
      "source": [
        "Road and Horizon Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cv003fjRqDXH"
      },
      "outputs": [],
      "source": [
        "bce_loss = nn.BCELoss(reduction='none')\n",
        "class RoadHorizonLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RoadHorizonLoss, self).__init__()\n",
        "    def forward(self, outputs:torch.tensor):\n",
        "        targets = [create_road_mask(outputs[i]).to(device).float() for i in range(batch_size)]\n",
        "        road_losses = bce_loss(outputs, torch.stack(targets, dim=0))\n",
        "        weighted_sum_losses = [road_losses[i] * (create_weight_mask(road_losses[i]).to(device)) for i in range(batch_size)]\n",
        "        loss = torch.mean(torch.stack(weighted_sum_losses, dim=0))\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozEn7HdZrzfR"
      },
      "source": [
        "TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_zf7VFr9fXny"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 1e-3\n",
        "criterion = RoadHorizonLoss()\n",
        "num_classes=2\n",
        "\n",
        "# params for ShiTomasi corner detection\n",
        "feature_params = dict(maxCorners=2000,\n",
        "                      qualityLevel=0.001,\n",
        "                      minDistance=5,\n",
        "                      blockSize=5)\n",
        "\n",
        "# Parameters for lucas kanade optical flow\n",
        "lk_params = dict(winSize=(35, 35),\n",
        "                 maxLevel=1,\n",
        "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.01))\n",
        "\n",
        "color = np.random.randint(0, 255, (1000, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vMYmpmkmATlt"
      },
      "outputs": [],
      "source": [
        "def track(img1,img2):\n",
        "    img1 = np.asarray(np.transpose(img1.cpu(), (1, 2, 0)))\n",
        "    img2 = np.asarray(np.transpose(img2.cpu(), (1, 2, 0)))\n",
        "\n",
        "    old_frame = np.ascontiguousarray(img1*255, dtype=np.uint8)\n",
        "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    frame = np.ascontiguousarray(img2*255, dtype=np.uint8)\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
        "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
        "\n",
        "    if p1 is not None: # Select good points being successfully tracked\n",
        "        good_new = p1[(st == 1) & (err < 20)]\n",
        "        good_old = p0[(st == 1) & (err < 20)]\n",
        "\n",
        "    h, w =205, 410\n",
        "    label_map = np.zeros((h, w), dtype=np.uint8)\n",
        "    points = np.array([[u1, v1], [u3, v3], [u4, v4], [u2, v2]])\n",
        "    label_map = cv2.fillPoly(label_map, pts=[points], color=(1, 0, 0))\n",
        "    new_points = []\n",
        "    old_points = []\n",
        "    all_errors = []\n",
        "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "        a, b = new.ravel()\n",
        "        c, d = old.ravel()\n",
        "        if b<205 and a<410 and d<205 and c<410 :\n",
        "            if (label_map[int(b), int(a)] == 1.0 and label_map[int(d), int(c)] == 1.0) or (b<horizon and d<horizon) or\\\n",
        "             a<0 or b<0 or c<0 or d<0:\n",
        "              continue\n",
        "        if b>205 or a>410 or d>205 or c>410 :\n",
        "              continue\n",
        "\n",
        "        new_points.append((int(b), int(a)))\n",
        "        old_points.append((int(d), int(c)))\n",
        "\n",
        "        good_errors = err[st == 1]\n",
        "        all_errors.extend(good_errors.flatten())\n",
        "\n",
        "    # print(f\"Error Range: {np.min(all_errors)} to {np.max(all_errors)}\")\n",
        "\n",
        "    return old_points, new_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gpWk0pBua6b"
      },
      "source": [
        "Show The Track\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "yInnivweEOGR"
      },
      "outputs": [],
      "source": [
        "def initial_mask(img: torch.tensor): #displays road and horizon mask\n",
        "  h,w = 205, 410\n",
        "  label_map = np.zeros((h,w), dtype=np.uint8)\n",
        "  color_map = np.zeros((h,w,3), dtype=np.uint8)\n",
        "\n",
        "  points = np.array([[u1, v1], [u3, v3], [u4, v4], [u2, v2]])\n",
        "  label_map = cv2.fillPoly(label_map, pts=[points], color=(1, 0, 0))\n",
        "\n",
        "  horizon_points = np.array([[0,0], [0, horizon], [w, horizon], [w, 0]])\n",
        "  label_map = cv2.fillPoly(label_map, pts=[horizon_points], color=(1, 0, 0))\n",
        "\n",
        "  color_map = np.where(label_map[:,:,None] == 1, (0,153,0), (0,0,0))\n",
        "  return color_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY5GWDctVLSH"
      },
      "source": [
        "SHOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB0VWjXFO8Dg"
      },
      "outputs": [],
      "source": [
        "for ind, images in enumerate(dataloader_train):\n",
        "  lists = track(images[0],images[batch_size-1])\n",
        "  mask = np.zeros((205, 410, 3)).astype(np.uint8)\n",
        "  for old, new in zip(lists[0], lists[1]):\n",
        "    old=(old[1], old[0])\n",
        "    new=(new[1], new[0])\n",
        "    colour = color[i].tolist()\n",
        "    cv2.line(mask[0:339, 0:423], new, old, colour, 1)\n",
        "    cv2.circle(mask[0:339, 0:423], new, 2, colour, -1)\n",
        "    cv2.circle(mask[0:339, 0:423], old, 2, colour, -1)\n",
        "\n",
        "  # Add track mask and rgb labels to image for showing\n",
        "  image = images[batch_size-1,:,:,:]\n",
        "  J = image.cpu().numpy()\n",
        "  J = np.transpose(J, (1, 2, 0))\n",
        "  J = np.ascontiguousarray(J*255, dtype=np.uint8)\n",
        "  J = cv2.add(J, mask)\n",
        "\n",
        "  mask2 = initial_mask(image)\n",
        "\n",
        "  J = J + (0.8* mask2.astype('float32'))\n",
        "  if ind%5 == 0:\n",
        "    cv2_imshow(J)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-SHBjwhhTvl"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = seg.lraspp_mobilenet_v3_large(pretrained=False)\n",
        "model.classifier.high_classifier = torch.nn.Conv2d(model.classifier.high_classifier.in_channels\n",
        "                                                   , num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "model.classifier.low_classifier = torch.nn.Conv2d(model.classifier.low_classifier.in_channels\n",
        "                                                   , num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "model_path=\"/content/300.torch\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model = model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_LCwpN-YpKk"
      },
      "source": [
        "Training using Mutual Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn7sTyTE0V9R"
      },
      "outputs": [],
      "source": [
        "#Invariant Information Clustering\n",
        "EPS = 1e-6\n",
        "def IIC(z, zt, C=2):\n",
        "    P = (z.unsqueeze(2) * zt.unsqueeze(1)).sum(dim=0)\n",
        "    P = ((P + P.t()) / 2) / P.sum()\n",
        "    P[(P < EPS).data] = EPS\n",
        "    Pi = P.sum(dim=1).view(C, 1).expand(C, C)\n",
        "    Pj = P.sum(dim=0).view(1, C).expand(C, C)\n",
        "    return (P * (torch.log(Pi) + torch.log(Pj) - torch.log(P))).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h7RSLvgU_wEm"
      },
      "outputs": [],
      "source": [
        "#Train Loop\n",
        "model.train()\n",
        "t1 = datetime.datetime.now()\n",
        "loss_list=[]\n",
        "for i, data in enumerate(dataloader_train):\n",
        "    globals()[f\"first_coord_{i}\"]=[]\n",
        "    globals()[f\"sec_coord_{i}\"]=[]\n",
        "\n",
        "for epoch in range(10):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        inputs = data.to(device)\n",
        "        outputs = model(inputs)['out']\n",
        "        pred_probab = F.softmax(outputs.float())\n",
        "        pred_lable_one = pred_probab[:, 1 ,:,:]\n",
        "\n",
        "        loss1 = criterion(pred_lable_one)\n",
        "\n",
        "        ###Paired Loss Calculation###\n",
        "        image1, image2 = inputs[0,:,:,:], inputs[batch_size-1,:,:,:]\n",
        "        coordinates = track(image1, image2)\n",
        "\n",
        "        globals()[f\"first_coord_{i}\"].extend(coordinates[0])\n",
        "        globals()[f\"sec_coord_{i}\"].extend(coordinates[1])\n",
        "\n",
        "        pred1, pred2 = pred_probab[0,:,:,:], pred_probab[batch_size-1,:,:,:]\n",
        "\n",
        "        points1_value = [pred1[:,j[0],j[1]].tolist() for j in globals()[f\"first_coord_{i}\"]]\n",
        "        points2_value = [pred2[:,k[0],k[1]].tolist() for k in globals()[f\"sec_coord_{i}\"]]\n",
        "\n",
        "        points1_value = torch.tensor(points1_value, requires_grad=True).to(device)\n",
        "        points2_value = torch.tensor(points2_value, requires_grad=True).to(device)\n",
        "\n",
        "        loss2 = IIC(points1_value, points2_value) #MI\n",
        "        loss = loss1 + loss2\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      t2 = datetime.datetime.now()\n",
        "      torch.save(model.state_dict(), f\"{300+epoch}.torch\")\n",
        "      print(f\"epoch {epoch}, step {i}, loss1: {loss1.item()}, loss2: {loss2.item()}, time: {t2}\")\n",
        "\n",
        "torch.save(model.state_dict(), f\"{300+epoch}.torch\")\n",
        "print(f\"elapsed time (m): {(t2 - t1).seconds/60}\") # 300epochs in total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aK6eF8ZDAHhL"
      },
      "outputs": [],
      "source": [
        "# !find . -name \"*2nd.torch\" -type f -delete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA_mM6--tit6"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bKYgYTGnHnx"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "color_map = [(  0,  0,  0) , (128, 0, 128) ]\n",
        "for ind, data in enumerate(dataloader_train):\n",
        "  images = data\n",
        "  images = images.to(device)\n",
        "  outputs = model(images)['out']\n",
        "  segm = torch.argmax(outputs, 1).cpu().detach().numpy()\n",
        "  labelmap=segm[batch_size-1,:,:]\n",
        "\n",
        "  # create rgb lable map\n",
        "  rgblabel = np.zeros((3, 205, 410), dtype=np.uint8)\n",
        "  for i in range(labelmap.shape[0]):\n",
        "    for j in range(labelmap.shape[1]):\n",
        "        color_index = labelmap[i, j]\n",
        "        rgblabel[:, i, j] = color_map[color_index]\n",
        "\n",
        "  image = images[batch_size-1,:,:,:]\n",
        "  J = image.cpu().numpy()\n",
        "  J = np.transpose(J, (1, 2, 0))\n",
        "  J = np.ascontiguousarray(J*255, dtype=np.uint8)\n",
        "\n",
        "  J = J  +  0.7* np.transpose(rgblabel.astype('float32'), (1, 2, 0))\n",
        "  cv2_imshow(J)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}